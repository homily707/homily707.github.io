[{"content":"building kubebuilder init --domain tutorial.kubebuilder.io --repo tutorial.kubebuilder.io/project kubebuilder create api --group batch k--version v1 --kind CronJob config/manager: launch your controllers as pods in the cluster config/rbac: permissions required to run your controllers under their own service account config/default contains a Kustomize base for launching the controller in a standard configuration. main Scheme, Kinds 映射到 Go types\nmanager, 监控 controller，配置 cache client\nApi domain tutorial.kubebuilder.io kubebuilder create api --group batch --version v1 --kind CronJob 对应的apiVersion就是 batch.tutorial.kubebuilder.io/v1 type.go spec 注释\n//+kubebuilder:validation:MinLength=0 //+kubebuilder:validation:Minimum=0 // +optional status kind,list +kubebuilder:object:root. 生成kind\nSchemeBuilder.Register 注册\ncontroller rbac reconcile reconcile 实现了 Reader Writer的接口，可以直接 get List create delete\n1 using client to fetch CR\nr.Get(ctx, req.NamespacedName, \u0026amp;cronJob) r.List(ctx, \u0026amp;childJobs, client.InNamespace(req.Namespace), client.MatchingFields{jobOwnerKey: req.Name}) 2 根据自己逻辑 控制CR\nFinalizers add finalizer后要 update delete object: predelete logic → delete finalizer → update object pre-delete must be idempotent Watch 自己控制的 Own controllerutil.SetControllerReference\nManager builder 模式添加 Owns\nexternal builder模式 添加 Watch\nWebhook ","permalink":"https://homily707.github.io/posts/k8s/kubebuilder/","summary":"building kubebuilder init --domain tutorial.kubebuilder.io --repo tutorial.kubebuilder.io/project kubebuilder create api --group batch k--version v1 --kind CronJob config/manager: launch your controllers as pods in the cluster config/rbac: permissions required to run your controllers under their own service account config/default contains a Kustomize base for launching the controller in a standard configuration. main Scheme, Kinds 映射到 Go types\nmanager, 监控 controller，配置 cache client\nApi domain tutorial.kubebuilder.io kubebuilder create api --group batch --version v1 --kind CronJob 对应的apiVersion就是 batch.","title":"Kubebuilder"},{"content":" 存储的设计 事务模型 查询引擎 复制 基础篇 外部视角 写多读少、低延时、高并发\n海量并发（分布式和单体的区别）\n高可靠\nrto 恢复时间 rpo 恢复点\n海量存储\n内部视角 客户端组件 + 单体 sharding jdbc 中间件 + 单体。 MyCat 应用层重构 + 单体 数据一致性 状态一致性\n强一致：mysql 全同步复制 弱一致：eventually consistency 操作一致性\n写后读 写入成功，异步复制，保证写入者能读到 单调读 读过的数据，不会消失。将用户和副本建立映射关系。 前缀 保证事件复制的因果关系 线性一致性 Linearizability 所有操作可以比较先后顺序。通过全局时钟建立全序关系。 线性一致性是描述历史记录的，而不是描述系统的。我们可以判断访问系统获取的一系列历史记录，来判断这个结果是不是线性一致，从而判断这个系统是否能实现线性一致。 因果一致 逻辑时钟，建立不那么准确的 全序关系 事务一致性 ACID\n一致性：整体目标\n持久性：数据丢失、故障容错。write ahead log， 多副本\n隔离性 异常现象 幻读： T1 查询两次， T2插入。T1第2次读结果集增大\n不可重复读： T1 查询两次， T2修改并提交。T1两次读结果不一致\n脏读： T1 查询两次， T2修改但未提交。T1两次读结果不一致\n隔离级别 未提交读： 脏读\n已提交读： 不可重复读\n可重复读： 会幻读\n快照隔离： write skew\n可串形化： 事务一个个执行\n架构 客户端通讯 查询处理器 事务管理器 进程管理器 辅助工具 单体数据库\n单体数据库（数据节点） + 协调节点 + 分片信息 + 全局时钟\nnewSql\n计算节点 + 存储节点 + 全局时钟 + 元数据管理\n时钟 spanner true time 物理时间 ，多时间源，多授时点\nTIDB TSO 中心授时 逻辑时钟。\nPD 组成raft组，提供授时服务，使用etcd + 预申请时间窗\n分片 hash 、 一致性hash。（无法处理业务热点，无法范围查询）\nrange 静态\nrange 动态\n动态分裂、合并 通过调度实现存储均衡、压力均衡。减少分布式事务 数据复制 元数据 TIKV 上node多个raft组存储分片信息。PD中心点读取kv心跳，获取当前分片信息。\n复制效率 raft的半数同意会导致效率低，且操作是按顺序apply的\nTIDB优化\nbatch，多个请求一起发送 pipeline，nextIndex parallelly，leader和follower的log并行apply apply，异步执行 分布式事务 原子性 TCC 应用层修改\n2PC 事务管理器，在多个节点上先分别prepare，确认后再commit\n问题：同步阻塞，事务管理器单点故障、commit失败数据不一致。\nPrecolator\n使用mvcc，新增版本。在新版本上加主锁和次锁，次锁指向主锁。\ncommit，删掉主锁即可。次锁记录，lazy 加载，异步删除\n事务延迟 延迟与写操作线性相关\n缓存提交写 隔离性 MVCC。分布式中通过全局事务管理器，维护递增事务id、提供全局快照\n存储方式\nappend-only 存储在表中 delta，独立存储增量 timetravel，独立存储全量 TIDB没有采用快照，读写阻塞\n不确定的时间窗口。\nspanner 写等待 cockroach 读等待\n悲观、乐观 乐观 rcvw 读 算 验证 写\n悲观 vrcw 验证 读 算 写 此处的读是拷贝加改写副本。写是提交\ntidb乐观锁，虽然每一行都悲观的。但是整体事务是没有有效性验证\n悲观，执行sql时，添加锁\n查询 存储过程\n难以调试\n自增主键\n连续递增，事务回滚时会失效 单调递增，并发过大时会失效 尾部热点， HTAP\nOLTP 和 OLAP 之间，需要ETL进行转换，存在时延。\nTiFlash存储分离，flash 作为raft的一个learner。每次查询前找leader实现同步。\n采用delta tree。\nKappa\nkafka加stream计算，快速加工\n计算下推 谓词下推\n分区索引\n全局索引\n查询引擎 火山模型\n向量化模型\n代码生成\nRUM read update memory 三者不能同时优化\nB tree 写放大，实际写比写的操作要多 存储不连续\nLSM tree Tiered\n先写内存，直接返回 内存 flush 进入 sstable sstable compact 合并 读放大，要同时读多个sstable\n写放大，compact操作性能消耗很高\nleveled compact\nL0 可重叠\n整理形成不重叠的L1\n然后部分compact\nWiscKey value单独存储\n","permalink":"https://homily707.github.io/posts/db/distribute-30/","summary":"存储的设计 事务模型 查询引擎 复制 基础篇 外部视角 写多读少、低延时、高并发\n海量并发（分布式和单体的区别）\n高可靠\nrto 恢复时间 rpo 恢复点\n海量存储\n内部视角 客户端组件 + 单体 sharding jdbc 中间件 + 单体。 MyCat 应用层重构 + 单体 数据一致性 状态一致性\n强一致：mysql 全同步复制 弱一致：eventually consistency 操作一致性\n写后读 写入成功，异步复制，保证写入者能读到 单调读 读过的数据，不会消失。将用户和副本建立映射关系。 前缀 保证事件复制的因果关系 线性一致性 Linearizability 所有操作可以比较先后顺序。通过全局时钟建立全序关系。 线性一致性是描述历史记录的，而不是描述系统的。我们可以判断访问系统获取的一系列历史记录，来判断这个结果是不是线性一致，从而判断这个系统是否能实现线性一致。 因果一致 逻辑时钟，建立不那么准确的 全序关系 事务一致性 ACID\n一致性：整体目标\n持久性：数据丢失、故障容错。write ahead log， 多副本\n隔离性 异常现象 幻读： T1 查询两次， T2插入。T1第2次读结果集增大\n不可重复读： T1 查询两次， T2修改并提交。T1两次读结果不一致\n脏读： T1 查询两次， T2修改但未提交。T1两次读结果不一致\n隔离级别 未提交读： 脏读","title":"分布式数据库笔记"},{"content":"原理 实现要点 show me the code ","permalink":"https://homily707.github.io/posts/algo/skiplist/","summary":"原理 实现要点 show me the code ","title":"跳表"},{"content":"flow\nlivelocks 系统没有阻塞。但是状态来回转换，没有进展。比如多节点同时竞选。\n当收到心跳、自己参与竞选、收到竞选申请时，三种情况下，都要重置election timeout 计时\nIncorrect RPC handler 如果reply false，快速结束，不要执行其余子过程 entries为null的rpc也要处理 日志处理的rule5是必要的 Failure to follow rule 确保apply只进行了一次 周期检查commit和apply或者每次commit时apply 如果append 由于term不一致被拒绝，不要更新nextIndex 不能更新以前term的commit Term confusion 当获得来自以前term的reply。如果term不同，不处理 2A：leader election 官方hints\nRequestVote 参加竞选 handler 参与投票。5秒内选出新leader AppendEntries 心跳。 每秒不超过十次 犯的几个错：\n忘了维护votedFor的值 心跳的间隔时间要确保小于随机check时间 旧主以及竞选失败者 收到心跳时，没有退化 旧主disconnects，进入election，但是没有竞选 leader 也给自己发了心跳 收到竞选，停止心跳检查，停止参加竞选 计票统计，没有并发控制 参与竞选，要并发执行 几个实现：\nlog 标准化，每次打印当前 raft的状态的，并用emoji来区分 log.SetFlags(log.Lmicroseconds) 可变参数的引用和解引用 func (rf Raft) Log(format string, args ...interface{}) { RaftPrintf(format, args...) } 2B：log 官方hints\n实现 election restriction 可能会出现多主的bug，请查看timer的实现或者不要立刻发心跳 通过condition 或者 sleep，不要让检查状态的进程死循环 犯的错\n锁重入会导致阻塞 chan一定要初始化 commit以后没有通知 有一个地方term 写成了index ","permalink":"https://homily707.github.io/posts/db/6.824-lab2-coding/","summary":"flow\nlivelocks 系统没有阻塞。但是状态来回转换，没有进展。比如多节点同时竞选。\n当收到心跳、自己参与竞选、收到竞选申请时，三种情况下，都要重置election timeout 计时\nIncorrect RPC handler 如果reply false，快速结束，不要执行其余子过程 entries为null的rpc也要处理 日志处理的rule5是必要的 Failure to follow rule 确保apply只进行了一次 周期检查commit和apply或者每次commit时apply 如果append 由于term不一致被拒绝，不要更新nextIndex 不能更新以前term的commit Term confusion 当获得来自以前term的reply。如果term不同，不处理 2A：leader election 官方hints\nRequestVote 参加竞选 handler 参与投票。5秒内选出新leader AppendEntries 心跳。 每秒不超过十次 犯的几个错：\n忘了维护votedFor的值 心跳的间隔时间要确保小于随机check时间 旧主以及竞选失败者 收到心跳时，没有退化 旧主disconnects，进入election，但是没有竞选 leader 也给自己发了心跳 收到竞选，停止心跳检查，停止参加竞选 计票统计，没有并发控制 参与竞选，要并发执行 几个实现：\nlog 标准化，每次打印当前 raft的状态的，并用emoji来区分 log.SetFlags(log.Lmicroseconds) 可变参数的引用和解引用 func (rf Raft) Log(format string, args ...interface{}) { RaftPrintf(format, args...) } 2B：log 官方hints\n实现 election restriction 可能会出现多主的bug，请查看timer的实现或者不要立刻发心跳 通过condition 或者 sleep，不要让检查状态的进程死循环 犯的错","title":"6.824 Lab2 Coding"},{"content":"replicated state machines 多个状态机保持相同的状态\n实现，通过日志，按顺序保存command。先复制，再执行\n非拜占庭问题下正常工作 多数节点可用时整体可用 paxos 缺点 难以理解、不好实现\n设计 分解、减少状态\nalgorithm basics 最少5个节点 三种节点 leader、follower、candidate 每一次选举都是一次term，选举失败也算一次。 每个节点存储当前 term，并在每次交流中带上，如果自己过时了（乃不知有汉，无论魏晋），则跟上最新的。leader和candidater 如果发现过时，退为follower RPC 有两种vote、 append 、 （retry） 选主 心跳 开始都是follower。leader会定期发送心跳（没有log的 appendRpc）\nelection timeout，follower长期没收到心跳，开始竞选。增加 term，切为 candidate，开始循环给所有人发送 voteRpc。会有3种结果 自己选上、别人选上、超时\n选举，每个节点投票给第一个找自己竞选的，参选者投自己。\n收到超过半数的投票，自己上任，给别人发心跳。如果收到竞选term的leader指令，代表自己失败。\n如果超时，增加term，重新竞选。\n防止多人参选，随机election timeout。candidate需要每次选举前，更新随机timeout\n日志复制 先群发 appendRpc。超过半数确认，然后commit。append中下发commited index 超时无响应，重发\n日志信息保存term\nappend 带上 上一次的entry，follower没找到这个的话，说明有漏，就不接受 主节点被拒绝后，会减小index\n当主节点出错时，新任leader强制以自己的log为准 主节点保存所有从的 nextIndex\nSafety 之前两节并不能保证各节点 exec same command in same order。比如一个落后很多的节点，timeout后竞选成为主节点然后执行后续命令会导致其他节点出错。\n这一节添加了选为主节点的限制。保证每个任期的leader都持有所有已提交的命令。 并且精细了commit的规则 最后，给出了证明\n5.4.1 election restriction 有主节点的共识算法，主节点必须存储所有已提交entries。\nRaft在不向主节点传递日志的情况下实现这一点。log流向只会从主节点发往其他\n在选举过程中，如果某个选举人的commit比别的落后，不可以当主\n日志比较。term高者最新，term相同，index高者最新\n5.4.2 commit from pervious term 新皇上位时，可能覆盖已经commit的entry。\n不是当前任期的日志，不通过数副本方式提交。如果拿到当前任期的commit日志，改日志之前的，全部提交\n","permalink":"https://homily707.github.io/posts/db/6.824-lab2-paper/","summary":"replicated state machines 多个状态机保持相同的状态\n实现，通过日志，按顺序保存command。先复制，再执行\n非拜占庭问题下正常工作 多数节点可用时整体可用 paxos 缺点 难以理解、不好实现\n设计 分解、减少状态\nalgorithm basics 最少5个节点 三种节点 leader、follower、candidate 每一次选举都是一次term，选举失败也算一次。 每个节点存储当前 term，并在每次交流中带上，如果自己过时了（乃不知有汉，无论魏晋），则跟上最新的。leader和candidater 如果发现过时，退为follower RPC 有两种vote、 append 、 （retry） 选主 心跳 开始都是follower。leader会定期发送心跳（没有log的 appendRpc）\nelection timeout，follower长期没收到心跳，开始竞选。增加 term，切为 candidate，开始循环给所有人发送 voteRpc。会有3种结果 自己选上、别人选上、超时\n选举，每个节点投票给第一个找自己竞选的，参选者投自己。\n收到超过半数的投票，自己上任，给别人发心跳。如果收到竞选term的leader指令，代表自己失败。\n如果超时，增加term，重新竞选。\n防止多人参选，随机election timeout。candidate需要每次选举前，更新随机timeout\n日志复制 先群发 appendRpc。超过半数确认，然后commit。append中下发commited index 超时无响应，重发\n日志信息保存term\nappend 带上 上一次的entry，follower没找到这个的话，说明有漏，就不接受 主节点被拒绝后，会减小index\n当主节点出错时，新任leader强制以自己的log为准 主节点保存所有从的 nextIndex\nSafety 之前两节并不能保证各节点 exec same command in same order。比如一个落后很多的节点，timeout后竞选成为主节点然后执行后续命令会导致其他节点出错。\n这一节添加了选为主节点的限制。保证每个任期的leader都持有所有已提交的命令。 并且精细了commit的规则 最后，给出了证明\n5.4.1 election restriction 有主节点的共识算法，主节点必须存储所有已提交entries。\nRaft在不向主节点传递日志的情况下实现这一点。log流向只会从主节点发往其他","title":"6.824 Lab2 Paper"},{"content":"guidance Read this guide for Raft specific advice. Advice on locking in labs. Advice on structuring your Raft lab. This Diagram of Raft interactions may help you understand code flow between different parts of the system. Dprint and colorful print debuging fault因 error 果 （隐藏的error， 出现的error，masked的error） 程序的三种状态\n程序正确，但是已经有 fault fault已经产生latent error。但是还未明显影响程序状态 error出现 做法\n想办法，减少阶段2持续的时间，所以就能更快的定位1切换到2的时候。 保持头脑清醒，聚焦当前的first observable error，不停的往前追溯。 bisection instrumentation\n要能方便地调节log的详细程度 可读性，颜色，保持一致的输出 加强新增日志能力 tips\n多个faults时，先找第一个 不要轻易排除，不要依赖你脑海中的模型，always verify your assumptions 复盘 不要做 不成熟的补救 fail loudly， offensive programming 偶发bug，疯狂的输出日志，坐等复现 先想想what the fuck the test is doing ","permalink":"https://homily707.github.io/posts/db/6.824-lab2-guide/","summary":"guidance Read this guide for Raft specific advice. Advice on locking in labs. Advice on structuring your Raft lab. This Diagram of Raft interactions may help you understand code flow between different parts of the system. Dprint and colorful print debuging fault因 error 果 （隐藏的error， 出现的error，masked的error） 程序的三种状态\n程序正确，但是已经有 fault fault已经产生latent error。但是还未明显影响程序状态 error出现 做法\n想办法，减少阶段2持续的时间，所以就能更快的定位1切换到2的时候。 保持头脑清醒，聚焦当前的first observable error，不停的往前追溯。 bisection instrumentation\n要能方便地调节log的详细程度 可读性，颜色，保持一致的输出 加强新增日志能力 tips\n多个faults时，先找第一个 不要轻易排除，不要依赖你脑海中的模型，always verify your assumptions 复盘 不要做 不成熟的补救 fail loudly， offensive programming 偶发bug，疯狂的输出日志，坐等复现 先想想what the fuck the test is doing ","title":"6.824 Lab2 Guide"},{"content":"实现 master有四个状态，不同的状态使用不同的处理逻辑。而worker是无状态的，直接干活job就行了，只用判断一下job的类型 map reduce的原理很好理解，直接实现即可 master用两个 chan 来保存当前要下发的map job 和 reduce job，用一个RWMap来管理job的状态。因为涉及并发，每次都要上锁。\n错误处理 job下发时记录下下发时间，收到job完成时要和下发时间进行比较。在Done函数中，定时遍历检查job状态，有超时的重新传入chan。\n库函数积累 json.NewDecoder(file)\njson.NewEncoder(tempFile)\ndec.Decode(\u0026amp;kv)\nioutil.TempFile\n","permalink":"https://homily707.github.io/posts/db/6.824-lab1/","summary":"实现 master有四个状态，不同的状态使用不同的处理逻辑。而worker是无状态的，直接干活job就行了，只用判断一下job的类型 map reduce的原理很好理解，直接实现即可 master用两个 chan 来保存当前要下发的map job 和 reduce job，用一个RWMap来管理job的状态。因为涉及并发，每次都要上锁。\n错误处理 job下发时记录下下发时间，收到job完成时要和下发时间进行比较。在Done函数中，定时遍历检查job状态，有超时的重新传入chan。\n库函数积累 json.NewDecoder(file)\njson.NewEncoder(tempFile)\ndec.Decode(\u0026amp;kv)\nioutil.TempFile","title":"6.824 Lab1"},{"content":" ","permalink":"https://homily707.github.io/posts/go/bubbletea/","summary":" ","title":"Bubbletea"},{"content":" ","permalink":"https://homily707.github.io/posts/db/tidb-0/","summary":" ","title":"Tidb-learning-0"},{"content":"原理理解 lowbit 就是数字的最低位 11000 -》 1000\n在树状数组中，我们用lowbit表示这个下标的sum，包含了几个数。比如 11010 的 lowbit 为 10，即他管辖了 （11000，11010] 这两个数的和，左开右闭。所以当我们要求【0，x】的和的时候，就是求[0，x-lowbit(x)]+（x-lowbit(x),x] ，后者我们已经存储好了，前者通过迭代的方式求出。因为是左开右闭，我们实际只能求到（0,x]，所以我们要能额外添加一个index为0的，值也为0的值。所以在实现的时候，真正的数字是从下标1开始的。\n一个数加上自己lowbit 11010 + 10 = 11100， 实现的就是最低位的进位，而这个进位所得到的数，一定是管辖自己的数。\n实现要点 sum数组的下标和原始数组的下标存在加1的关系 lowbit 用 x\u0026amp;-x实现 show me the code type BinaryIndexTree struct { Data []int SumSlice []int Len int } func (bit *BinaryIndexTree) Init(array []int) { bit.Len = len(array) bit.Data = array bit.SumSlice = make([]int, bit.Len+1) for i, num := range array { bit.addInSum(i, num) } } func (bit *BinaryIndexTree) Add(index int, delta int) { bit.Data[index] += delta bit.addInSum(index, delta) } func (bit *BinaryIndexTree) Sum(left int, right int) int { return bit.getPreSum(right-1) - bit.getPreSum(left-1) } func (bit *BinaryIndexTree) addInSum(index int, delta int) { index++ for index \u0026lt;= bit.Len { bit.SumSlice[index] += delta index += lowbit(index) } } func (bit BinaryIndexTree) getPreSum(pos int) int { index := pos + 1 sum := 0 for index \u0026gt; 0 { sum += bit.SumSlice[index] index -= lowbit(index) } return sum } func lowbit(x int) int { return x \u0026amp; (-x) } ","permalink":"https://homily707.github.io/posts/algo/binaryindextree/","summary":"原理理解 lowbit 就是数字的最低位 11000 -》 1000\n在树状数组中，我们用lowbit表示这个下标的sum，包含了几个数。比如 11010 的 lowbit 为 10，即他管辖了 （11000，11010] 这两个数的和，左开右闭。所以当我们要求【0，x】的和的时候，就是求[0，x-lowbit(x)]+（x-lowbit(x),x] ，后者我们已经存储好了，前者通过迭代的方式求出。因为是左开右闭，我们实际只能求到（0,x]，所以我们要能额外添加一个index为0的，值也为0的值。所以在实现的时候，真正的数字是从下标1开始的。\n一个数加上自己lowbit 11010 + 10 = 11100， 实现的就是最低位的进位，而这个进位所得到的数，一定是管辖自己的数。\n实现要点 sum数组的下标和原始数组的下标存在加1的关系 lowbit 用 x\u0026amp;-x实现 show me the code type BinaryIndexTree struct { Data []int SumSlice []int Len int } func (bit *BinaryIndexTree) Init(array []int) { bit.Len = len(array) bit.Data = array bit.SumSlice = make([]int, bit.Len+1) for i, num := range array { bit.addInSum(i, num) } } func (bit *BinaryIndexTree) Add(index int, delta int) { bit.","title":"树状数组"}]