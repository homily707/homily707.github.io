[{"content":" work queue Queue\nqueue []t // 定义元素的处理顺序，里面所有元素都应该在 dirty set 中有，而不能出现在 processing set 中 dirty set // 标记所有需要被处理的元素 processing set // 当前正在被处理的元素，当处理完后需要检查该元素是否在 dirty set 中，如果有则添加到 queue 里 DelayingQueue 多了一个AddAfter，每次add加入堆中，每次拿出最早的\nRateLimitingQueue。限速队列\nDelta FIFO 接口queue\n接口store\nitems map[string]Delta\nDelta. 五种type added updated deleted replaced sync . 带一个 interface{}\nPop方法：会阻塞。传入的是一个函数，如果执行失败重新入队\nIndexer type Indexer interface { Store Index(indexName string, obj interface{}) ([]interface{}, error) // 根据索引名和给定的对象返回符合条件的所有对象 IndexKeys(indexName, indexedValue string) ([]string, error) // 根据索引名和索引值返回符合条件的所有对象的 key ListIndexFuncValues(indexName string) []string // 列出索引函数计算出来的所有索引值 ByIndex(indexName, indexedValue string) ([]interface{}, error) // 根据索引名和索引值返回符合条件的所有对象 GetIndexers() Indexers // 获取所有的 Indexers，对应 map[string]IndexFunc 类型 AddIndexers(newIndexers Indexers) error // 这个方法要在数据加入存储前调用，添加更多的索引方法，默认只通过 namespace 检索 } type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) Replace([]interface{}, string) error Resync() error } // 默认实现类 type cache struct { cacheStorage ThreadSafeStore keyFunc KeyFunc } keyfunc 计算 object的key，然后用ThreadSafeStore存储key：object\nstore的实现 type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} // indexers maps a name to an IndexFunc indexers Indexers // indices maps a name to an Index indices Indices } type Index map[string]sets.String type Indexers map[string]IndexFunc type Indices map[string]Index type IndexFunc func(obj interface{}) ([]string, error) 举例：\nfunc testUsersIndexFunc(obj interface{}) ([]string, error) { pod := obj.(*v1.Pod) usersString := pod.Annotations[\u0026#34;users\u0026#34;] return strings.Split(usersString, \u0026#34;,\u0026#34;), nil } func TestMultiIndexKeys(t *testing.T) { index := NewIndexer(MetaNamespaceKeyFunc, Indexers{\u0026#34;byUser\u0026#34;: testUsersIndexFunc}) pod1 := \u0026amp;v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;one\u0026#34;, Annotations: map[string]string{\u0026#34;users\u0026#34;: \u0026#34;ernie,bert\u0026#34;}}} pod2 := \u0026amp;v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;two\u0026#34;, Annotations: map[string]string{\u0026#34;users\u0026#34;: \u0026#34;bert,oscar\u0026#34;}}} pod3 := \u0026amp;v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: \u0026#34;tre\u0026#34;, Annotations: map[string]string{\u0026#34;users\u0026#34;: \u0026#34;ernie,elmo\u0026#34;}}} } item 存储的就是3个pod， indexer 存储了一个func，这个func会将pod的users split成多个并返回 indices 存储了一个 index，其中ernie 对应了2个key，这个key用于item查询\n看到这里就会恍然大悟，这其实就是一个倒排索引。当func多了以后，就能实现非常强悍的功能了。\nListWatcher 经典的 接口组合\nlist 和 watch 直接调用 restClient 这里额外需要一个Getter 来返回 rest request。 然后使用闭包函数的形式，把getter传给restClient getter的实现类 在 rest/client.go 中，经典的builder模式\ntype Lister interface { List(options metav1.ListOptions) (runtime.Object, error) } type Watcher interface { Watch(options metav1.ListOptions) (watch.Interface, error) } // ListerWatcher is any object that knows how to perform an initial list and start a watch on a resource. type ListerWatcher interface { Lister Watcher } type Getter interface { Get() *restclient.Request } func NewListWatchFromClient(c Getter, resource string, namespace string, fieldSelector fields.Selector) *ListWatch { optionsModifier := func(options *metav1.ListOptions) { options.FieldSelector = fieldSelector.String() } return NewFilteredListWatchFromClient(c, resource, namespace, optionsModifier) } Reflector 健壮性保证\nlist ，传一个sync 到 DeltaFIFO\nwatch，watchHandler 处理后 传入 DeltaFIFO\ninformer type Controller interface { // Run does two things. One is to construct and run a Reflector // to pump objects/notifications from the Config\u0026#39;s ListerWatcher // to the Config\u0026#39;s Queue and possibly invoke the occasional Resync // on that Queue. The other is to repeatedly Pop from the Queue // and process with the Config\u0026#39;s ProcessFunc. Both of these // continue until `stopCh` is closed. Run(stopCh \u0026lt;-chan struct{}) HasSynced() bool LastSyncResourceVersion() string } type controller struct { config Config reflector *Reflector reflectorMutex sync.RWMutex clock clock.Clock } type Config struct { // The queue for your objects - has to be a DeltaFIFO due to // assumptions in the implementation. Your Process() function // should accept the output of this Queue\u0026#39;s Pop() method. Queue // Something that can list and watch your objects. ListerWatcher // Something that can process a popped Deltas. Process ProcessFunc ObjectType runtime.Object FullResyncPeriod time.Duration RetryOnError bool WatchErrorHandler WatchErrorHandler WatchListPageSize int64 } type ResourceEventHandler interface { OnAdd(obj interface{}) OnUpdate(oldObj, newObj interface{}) OnDelete(obj interface{}) } 真正的处理逻辑 就是 config里的Process\nSharedIndexInformer type sharedIndexInformer struct { indexer Indexer controller Controller processor *sharedProcessor cacheMutationDetector MutationDetector listerWatcher ListerWatcher // 表示当前 Informer 期望关注的类型，主要是 GVK 信息 objectType runtime.Object // reflector 的 resync 计时器计时间隔，通知所有的 listener 执行 resync resyncCheckPeriod time.Duration defaultEventHandlerResyncPeriod time.Duration clock clock.Clock started, stopped bool startedLock sync.Mutex blockDeltas sync.Mutex watchErrorHandler WatchErrorHandler } type sharedProcessor struct { listenersStarted bool listenersLock sync.RWMutex listeners []*processorListener syncingListeners []*processorListener clock clock.Clock wg wait.Group } type processorListener struct { nextCh chan interface{} addCh chan interface{} // 核心属性 handler ResourceEventHandler pendingNotifications buffer.RingGrowing requestedResyncPeriod time.Duration resyncPeriod time.Duration nextResync time.Time resyncLock sync.Mutex } 核心就是对delta的处理\nfunc (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() if deltas, ok := obj.(Deltas); ok { // 把自己作为 event handler 传进去 return processDeltas(s, s.indexer, s.transform, deltas) } return errors.New(\u0026#34;object given as Process argument is not Deltas\u0026#34;) } func processDeltas( // Object which receives event notifications from the given deltas handler ResourceEventHandler, clientState Store, transformer TransformFunc, deltas Deltas, ) ","permalink":"https://homily707.github.io/posts/k8s/client-go/","summary":"work queue Queue\nqueue []t // 定义元素的处理顺序，里面所有元素都应该在 dirty set 中有，而不能出现在 processing set 中 dirty set // 标记所有需要被处理的元素 processing set // 当前正在被处理的元素，当处理完后需要检查该元素是否在 dirty set 中，如果有则添加到 queue 里 DelayingQueue 多了一个AddAfter，每次add加入堆中，每次拿出最早的\nRateLimitingQueue。限速队列\nDelta FIFO 接口queue\n接口store\nitems map[string]Delta\nDelta. 五种type added updated deleted replaced sync . 带一个 interface{}\nPop方法：会阻塞。传入的是一个函数，如果执行失败重新入队\nIndexer type Indexer interface { Store Index(indexName string, obj interface{}) ([]interface{}, error) // 根据索引名和给定的对象返回符合条件的所有对象 IndexKeys(indexName, indexedValue string) ([]string, error) // 根据索引名和索引值返回符合条件的所有对象的 key ListIndexFuncValues(indexName string) []string // 列出索引函数计算出来的所有索引值 ByIndex(indexName, indexedValue string) ([]interface{}, error) // 根据索引名和索引值返回符合条件的所有对象 GetIndexers() Indexers // 获取所有的 Indexers，对应 map[string]IndexFunc 类型 AddIndexers(newIndexers Indexers) error // 这个方法要在数据加入存储前调用，添加更多的索引方法，默认只通过 namespace 检索 } type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) Replace([]interface{}, string) error Resync() error } // 默认实现类 type cache struct { cacheStorage ThreadSafeStore keyFunc KeyFunc } keyfunc 计算 object的key，然后用ThreadSafeStore存储key：object","title":"Client Go"},{"content":"flow\nlivelocks 系统没有阻塞。但是状态来回转换，没有进展。比如多节点同时竞选。\n当收到心跳、自己参与竞选、收到竞选申请时，三种情况下，都要重置election timeout 计时\nIncorrect RPC handler 如果reply false，快速结束，不要执行其余子过程 entries为null的rpc也要处理 日志处理的rule5是必要的 Failure to follow rule 确保apply只进行了一次 周期检查commit和apply或者每次commit时apply 如果append 由于term不一致被拒绝，不要更新nextIndex 不能更新以前term的commit Term confusion 当获得来自以前term的reply。如果term不同，不处理 2A：leader election 官方hints\nRequestVote 参加竞选 handler 参与投票。5秒内选出新leader AppendEntries 心跳。 每秒不超过十次 犯的几个错：\n忘了维护votedFor的值 心跳的间隔时间要确保小于随机check时间 旧主以及竞选失败者 收到心跳时，没有退化 旧主disconnects，进入election，但是没有竞选 leader 也给自己发了心跳 收到竞选，停止心跳检查，停止参加竞选 计票统计，没有并发控制 参与竞选，要并发执行 几个实现：\nlog 标准化，每次打印当前 raft的状态的，并用emoji来区分 log.SetFlags(log.Lmicroseconds) 可变参数的引用和解引用 func (rf Raft) Log(format string, args ...interface{}) { RaftPrintf(format, args...) } 2B：log 官方hints\n实现 election restriction 可能会出现多主的bug，请查看timer的实现或者不要立刻发心跳 通过condition 或者 sleep，不要让检查状态的进程死循环 犯的错\n锁重入会导致阻塞 chan一定要初始化 commit以后没有通知 有一个地方term 写成了index ","permalink":"https://homily707.github.io/posts/db/6.824-lab2-coding/","summary":"flow\nlivelocks 系统没有阻塞。但是状态来回转换，没有进展。比如多节点同时竞选。\n当收到心跳、自己参与竞选、收到竞选申请时，三种情况下，都要重置election timeout 计时\nIncorrect RPC handler 如果reply false，快速结束，不要执行其余子过程 entries为null的rpc也要处理 日志处理的rule5是必要的 Failure to follow rule 确保apply只进行了一次 周期检查commit和apply或者每次commit时apply 如果append 由于term不一致被拒绝，不要更新nextIndex 不能更新以前term的commit Term confusion 当获得来自以前term的reply。如果term不同，不处理 2A：leader election 官方hints\nRequestVote 参加竞选 handler 参与投票。5秒内选出新leader AppendEntries 心跳。 每秒不超过十次 犯的几个错：\n忘了维护votedFor的值 心跳的间隔时间要确保小于随机check时间 旧主以及竞选失败者 收到心跳时，没有退化 旧主disconnects，进入election，但是没有竞选 leader 也给自己发了心跳 收到竞选，停止心跳检查，停止参加竞选 计票统计，没有并发控制 参与竞选，要并发执行 几个实现：\nlog 标准化，每次打印当前 raft的状态的，并用emoji来区分 log.SetFlags(log.Lmicroseconds) 可变参数的引用和解引用 func (rf Raft) Log(format string, args ...interface{}) { RaftPrintf(format, args...) } 2B：log 官方hints\n实现 election restriction 可能会出现多主的bug，请查看timer的实现或者不要立刻发心跳 通过condition 或者 sleep，不要让检查状态的进程死循环 犯的错","title":"6.824 Lab2 Coding"},{"content":"replicated state machines 多个状态机保持相同的状态\n实现，通过日志，按顺序保存command。先复制，再执行\n非拜占庭问题下正常工作 多数节点可用时整体可用 paxos 缺点 难以理解、不好实现\n设计 分解、减少状态\nalgorithm basics 最少5个节点 三种节点 leader、follower、candidate 每一次选举都是一次term，选举失败也算一次。 每个节点存储当前 term，并在每次交流中带上，如果自己过时了（乃不知有汉，无论魏晋），则跟上最新的。leader和candidater 如果发现过时，退为follower RPC 有两种vote、 append 、 （retry） 选主 心跳 开始都是follower。leader会定期发送心跳（没有log的 appendRpc）\nelection timeout，follower长期没收到心跳，开始竞选。增加 term，切为 candidate，开始循环给所有人发送 voteRpc。会有3种结果 自己选上、别人选上、超时\n选举，每个节点投票给第一个找自己竞选的，参选者投自己。\n收到超过半数的投票，自己上任，给别人发心跳。如果收到竞选term的leader指令，代表自己失败。\n如果超时，增加term，重新竞选。\n防止多人参选，随机election timeout。candidate需要每次选举前，更新随机timeout\n日志复制 先群发 appendRpc。超过半数确认，然后commit。append中下发commited index 超时无响应，重发\n日志信息保存term\nappend 带上 上一次的entry，follower没找到这个的话，说明有漏，就不接受 主节点被拒绝后，会减小index\n当主节点出错时，新任leader强制以自己的log为准 主节点保存所有从的 nextIndex\nSafety 之前两节并不能保证各节点 exec same command in same order。比如一个落后很多的节点，timeout后竞选成为主节点然后执行后续命令会导致其他节点出错。\n这一节添加了选为主节点的限制。保证每个任期的leader都持有所有已提交的命令。 并且精细了commit的规则 最后，给出了证明\n5.4.1 election restriction 有主节点的共识算法，主节点必须存储所有已提交entries。\nRaft在不向主节点传递日志的情况下实现这一点。log流向只会从主节点发往其他\n在选举过程中，如果某个选举人的commit比别的落后，不可以当主\n日志比较。term高者最新，term相同，index高者最新\n5.4.2 commit from pervious term 新皇上位时，可能覆盖已经commit的entry。\n不是当前任期的日志，不通过数副本方式提交。如果拿到当前任期的commit日志，改日志之前的，全部提交\n","permalink":"https://homily707.github.io/posts/db/6.824-lab2-paper/","summary":"replicated state machines 多个状态机保持相同的状态\n实现，通过日志，按顺序保存command。先复制，再执行\n非拜占庭问题下正常工作 多数节点可用时整体可用 paxos 缺点 难以理解、不好实现\n设计 分解、减少状态\nalgorithm basics 最少5个节点 三种节点 leader、follower、candidate 每一次选举都是一次term，选举失败也算一次。 每个节点存储当前 term，并在每次交流中带上，如果自己过时了（乃不知有汉，无论魏晋），则跟上最新的。leader和candidater 如果发现过时，退为follower RPC 有两种vote、 append 、 （retry） 选主 心跳 开始都是follower。leader会定期发送心跳（没有log的 appendRpc）\nelection timeout，follower长期没收到心跳，开始竞选。增加 term，切为 candidate，开始循环给所有人发送 voteRpc。会有3种结果 自己选上、别人选上、超时\n选举，每个节点投票给第一个找自己竞选的，参选者投自己。\n收到超过半数的投票，自己上任，给别人发心跳。如果收到竞选term的leader指令，代表自己失败。\n如果超时，增加term，重新竞选。\n防止多人参选，随机election timeout。candidate需要每次选举前，更新随机timeout\n日志复制 先群发 appendRpc。超过半数确认，然后commit。append中下发commited index 超时无响应，重发\n日志信息保存term\nappend 带上 上一次的entry，follower没找到这个的话，说明有漏，就不接受 主节点被拒绝后，会减小index\n当主节点出错时，新任leader强制以自己的log为准 主节点保存所有从的 nextIndex\nSafety 之前两节并不能保证各节点 exec same command in same order。比如一个落后很多的节点，timeout后竞选成为主节点然后执行后续命令会导致其他节点出错。\n这一节添加了选为主节点的限制。保证每个任期的leader都持有所有已提交的命令。 并且精细了commit的规则 最后，给出了证明\n5.4.1 election restriction 有主节点的共识算法，主节点必须存储所有已提交entries。\nRaft在不向主节点传递日志的情况下实现这一点。log流向只会从主节点发往其他","title":"6.824 Lab2 Paper"},{"content":"guidance Read this guide for Raft specific advice. Advice on locking in labs. Advice on structuring your Raft lab. This Diagram of Raft interactions may help you understand code flow between different parts of the system. Dprint and colorful print debuging fault因 error 果 （隐藏的error， 出现的error，masked的error） 程序的三种状态\n程序正确，但是已经有 fault fault已经产生latent error。但是还未明显影响程序状态 error出现 做法\n想办法，减少阶段2持续的时间，所以就能更快的定位1切换到2的时候。 保持头脑清醒，聚焦当前的first observable error，不停的往前追溯。 bisection instrumentation\n要能方便地调节log的详细程度 可读性，颜色，保持一致的输出 加强新增日志能力 tips\n多个faults时，先找第一个 不要轻易排除，不要依赖你脑海中的模型，always verify your assumptions 复盘 不要做 不成熟的补救 fail loudly， offensive programming 偶发bug，疯狂的输出日志，坐等复现 先想想what the fuck the test is doing ","permalink":"https://homily707.github.io/posts/db/6.824-lab2-guide/","summary":"guidance Read this guide for Raft specific advice. Advice on locking in labs. Advice on structuring your Raft lab. This Diagram of Raft interactions may help you understand code flow between different parts of the system. Dprint and colorful print debuging fault因 error 果 （隐藏的error， 出现的error，masked的error） 程序的三种状态\n程序正确，但是已经有 fault fault已经产生latent error。但是还未明显影响程序状态 error出现 做法\n想办法，减少阶段2持续的时间，所以就能更快的定位1切换到2的时候。 保持头脑清醒，聚焦当前的first observable error，不停的往前追溯。 bisection instrumentation\n要能方便地调节log的详细程度 可读性，颜色，保持一致的输出 加强新增日志能力 tips\n多个faults时，先找第一个 不要轻易排除，不要依赖你脑海中的模型，always verify your assumptions 复盘 不要做 不成熟的补救 fail loudly， offensive programming 偶发bug，疯狂的输出日志，坐等复现 先想想what the fuck the test is doing ","title":"6.824 Lab2 Guide"},{"content":" 入门Prometheus 架构原理 PromQl grafana 实战 slo metrics exporter microMeters 告警 其他 长尾效应 pushgateway exporter 入门prometheus 架构原理 Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。grafana通过PromQL查询数据，配置相关的可视化界面。AlertManager负责发出告警。 metrics 来源\n来自宿主机的 Node Exporter， 来自API Server的数据监控controller的相关信息 kubelet cAdvisor，k8s核心数据，pod，container cpu等 组件 exporter PromQL 数据类型 counter 只增不减 gauage 可增可减 histogram 分 bucket summary 显示百分位数 筛选 {}通过label 筛选 类似sql的 where。支持正则 返回一个一维数组，瞬时向量 http_requests_total{job=\u0026ldquo;prometheus\u0026rdquo;,group=\u0026ldquo;canary\u0026rdquo;}\n向量 [ ] 时间段查询range，返回的是一个二维数组，区间向量。往往需要配合函数使用 http_requests_total{job=\u0026ldquo;prometheus\u0026rdquo;}[5m]\n操作符 两个向量之间可以直接 相加相乘相除，但这里需要注意匹配模式。默认是1对1匹配。如果要1对多和多对1，需要额外处理。\n聚合操作 聚合操作有些用于瞬时向量 类似于 sql 的 group 比如 sum(http_requests_total) by (code,handler,job,method) 常用的还有 max,min,avg, count_values topk quantile 有些用于区间向量 ，常用 increase、rate、irate 这里有2篇文章详解rate https://www.metricfire.com/blog/understanding-the-prometheus-rate-function/ https://segmentfault.com/a/1190000040595000\n安装 grafana echo \u0026ldquo;Password: $(kubectl get secret my-grafana-admin \u0026ndash;namespace default -o jsonpath=\u0026rdquo;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026quot; | base64 -d)\u0026quot;\n实战 设定 SLO 谷歌提供的一个 VALET 例子 Avaliablity 可用性 SLO SLO \u0026gt; 99.9% 从请求成功占比的角度出发计算可用性。但同时也要关注集群、pod异常、前端报错的情况。\nhttp状态码200，但是有报错。在应用内部，用aop记录。 http状态码500。在应用内，用filter进行记录。 前端错误。nginx exporter。 应用异常。k8s 层面进行监控。 节点异常。k8s报警，人工处理。 这里我们采用http_server_requests 计算 状态码5.. 或者 状态码200 但是 有报错的情况。\nsum (rate(http_server_requests_seconds_count{exception=\u0026#34;None\u0026#34;,status!~\u0026#34;5..\u0026#34;,job=\u0026#34;dps-gateway\u0026#34;,uri!~\u0026#34;/probe/.*\u0026#34;}[7d]) ) by (job) / (sum(rate(http_server_requests_seconds_count{job=\u0026#34;dps-gateway\u0026#34;,uri!~\u0026#34;/probe/.*\u0026#34;}[7d])) by (job) ) Errors 错误 SLO 根据可用性slo设定的目标，设定错误预算 error budget，提示当前还有多少犯错机会\n24 * 7 * (sum (rate(http_server_requests_seconds_count{exception=\u0026#34;None\u0026#34;,status!~\u0026#34;5..\u0026#34;}[7d]) ) by (job) / (sum(rate(http_server_requests_seconds_count[7d])) by (job) ) * 100 - 99 ) / 100 Volume 容量 SLO **计算qps，每天最大容量是多少 ** sum(rate(dps_request_code_total[10m])) by (job)\nLatency 时延 SLO **计算RT ** sum (rate(http_server_requests_seconds_sum[10m]) ) by (job) / (sum(rate(http_server_requests_seconds_count[10m])) by (job) ) * 1000\nmicroMeters springboot的 spring-actutor已经继承了micrometer，我们可用直接集成。并且micrometer支持对接prometheus，非常方便。这里有篇文章详细的介绍。 spring-actutor已经基于 microMeter 内置了以下监控，对接prometheus后就可以直接使用\nhttp jvm hikari tomcat 对接prometheus 也非常简单，在配置好了microMeters的条件下。直接引入依赖。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.micrometer\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;micrometer-registry-prometheus\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; ======= management.endpoints.web.exposure.include=* management.endpoint.prometheus.enabled=true 引入了microMeter以后，我就可以通过aop切面，拦截controller方法，使用Timer统计出各个controller方法的时间，并捕获错误信息。注意如果有些错误filter直接拦截的话，就没法捕获了。\nexporter nginx exporter\nmysql exporter redis exporter\nalert 效果 全局 SLO， 错误预算，全天qps统计\n待做 pushgateway 自建exporter ","permalink":"https://homily707.github.io/posts/k8s/prometheus/","summary":"入门Prometheus 架构原理 PromQl grafana 实战 slo metrics exporter microMeters 告警 其他 长尾效应 pushgateway exporter 入门prometheus 架构原理 Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。grafana通过PromQL查询数据，配置相关的可视化界面。AlertManager负责发出告警。 metrics 来源\n来自宿主机的 Node Exporter， 来自API Server的数据监控controller的相关信息 kubelet cAdvisor，k8s核心数据，pod，container cpu等 组件 exporter PromQL 数据类型 counter 只增不减 gauage 可增可减 histogram 分 bucket summary 显示百分位数 筛选 {}通过label 筛选 类似sql的 where。支持正则 返回一个一维数组，瞬时向量 http_requests_total{job=\u0026ldquo;prometheus\u0026rdquo;,group=\u0026ldquo;canary\u0026rdquo;}\n向量 [ ] 时间段查询range，返回的是一个二维数组，区间向量。往往需要配合函数使用 http_requests_total{job=\u0026ldquo;prometheus\u0026rdquo;}[5m]\n操作符 两个向量之间可以直接 相加相乘相除，但这里需要注意匹配模式。默认是1对1匹配。如果要1对多和多对1，需要额外处理。\n聚合操作 聚合操作有些用于瞬时向量 类似于 sql 的 group 比如 sum(http_requests_total) by (code,handler,job,method) 常用的还有 max,min,avg, count_values topk quantile 有些用于区间向量 ，常用 increase、rate、irate 这里有2篇文章详解rate https://www.","title":"Prometheus"},{"content":"实现 master有四个状态，不同的状态使用不同的处理逻辑。而worker是无状态的，直接干活job就行了，只用判断一下job的类型 map reduce的原理很好理解，直接实现即可 master用两个 chan 来保存当前要下发的map job 和 reduce job，用一个RWMap来管理job的状态。因为涉及并发，每次都要上锁。\n错误处理 job下发时记录下下发时间，收到job完成时要和下发时间进行比较。在Done函数中，定时遍历检查job状态，有超时的重新传入chan。\n库函数积累 json.NewDecoder(file)\njson.NewEncoder(tempFile)\ndec.Decode(\u0026amp;kv)\nioutil.TempFile\n","permalink":"https://homily707.github.io/posts/db/6.824-lab1/","summary":"实现 master有四个状态，不同的状态使用不同的处理逻辑。而worker是无状态的，直接干活job就行了，只用判断一下job的类型 map reduce的原理很好理解，直接实现即可 master用两个 chan 来保存当前要下发的map job 和 reduce job，用一个RWMap来管理job的状态。因为涉及并发，每次都要上锁。\n错误处理 job下发时记录下下发时间，收到job完成时要和下发时间进行比较。在Done函数中，定时遍历检查job状态，有超时的重新传入chan。\n库函数积累 json.NewDecoder(file)\njson.NewEncoder(tempFile)\ndec.Decode(\u0026amp;kv)\nioutil.TempFile","title":"6.824 Lab1"},{"content":"TCP bufio.Reader 带缓冲的 reader.ReadString(\u0026quot;\\n\u0026quot;). 读到分隔符为止\n每个连接 goroutine 处理， waitGroup.add 并且 defer waitGroup.Done ， 主routine出错后也要 waitGroup.wait所有连接处理完\nsignal.Notify 监听退出信号 并用 chan struct{}. 与主routine通信。主routine 使用\u0026lt;-chan 保证通道只读\n粘包\nMap 分段锁， 每一个shard内用RwLock。进阶渐进式rehash\n操作原子性保证，同时给多个shard上锁。不delete锁，以免并发竞争。避免死锁，先排序shard，保证多个锁获取时，必须按顺序。\nAOF 过期时间的逻辑处理\n每次set的时候给通道发消息，异步routine处理\naof重写：redis 用的fork。godis用的aof生成副本，和ddl的异步更新有相同的意思\nZSet zset的实现。span实现rank，head虚节点作为首节点\ntype Level struct { forward *node // forward node has greater score span int64 } type node struct { Element backward *node level []*Level // level[0] is base level } type skiplist struct { header *node tail *node length int64 level int16 } Pipeline client 两个chan\npending ，存放req然后循环发送 waiting 存放返回值，通过 wait.wait 实现超时报错，然后解析 再来一个waitgroup，退出时也要处理完再退出 Cluster 一致性哈希\nTCC 分布式事务\ntype Transaction struct { id string // 事务 ID, 由协调者使用 snowflake 算法生成 cmdLine CmdLine // 事务要执行命令行 cluster *Cluster conn redis.Connection dbIndex int writeKeys []string // 事务要进行写入的 Key readKeys []string // 事务要进行读取的 Key keysLocked bool undoLog []CmdLine // 回滚命令 status int8 mu *sync.Mutex } 收到prepare ，执行并生成 undoLog 和 undoFunc\nGeoHash 分形\nfunc encode0(latitude, longitude float64, bitSize uint) ([]byte, [2][2]float64) { box := [2][2]float64{ {-180, 180}, // lng {-90, 90}, // lat } pos := [2]float64{longitude, latitude} hash := \u0026amp;bytes.Buffer{} bit := 0 var precision uint = 0 code := uint8(0) for precision \u0026lt; bitSize { for direction, val := range pos { mid := (box[direction][0] + box[direction][1]) / 2 if val \u0026lt; mid { box[direction][1] = mid // 编码默认为0，不需要操作 } else { box[direction][0] = mid code |= bits[bit] // 通过位或操作写入1，比如要在字节的第3位写入1应该 code |= 32 } bit++ if bit == 8 { // 计算完一个字节的编码，将其写入buffer hash.WriteByte(code) bit = 0 code = 0 } precision++ if precision == bitSize { break } } } // precision 可能无法被 8 整除，此时剩下的二进制编码写到最后 if code \u0026gt; 0 { hash.WriteByte(code) } return hash.Bytes(), box } ","permalink":"https://homily707.github.io/posts/db/godis/","summary":"TCP bufio.Reader 带缓冲的 reader.ReadString(\u0026quot;\\n\u0026quot;). 读到分隔符为止\n每个连接 goroutine 处理， waitGroup.add 并且 defer waitGroup.Done ， 主routine出错后也要 waitGroup.wait所有连接处理完\nsignal.Notify 监听退出信号 并用 chan struct{}. 与主routine通信。主routine 使用\u0026lt;-chan 保证通道只读\n粘包\nMap 分段锁， 每一个shard内用RwLock。进阶渐进式rehash\n操作原子性保证，同时给多个shard上锁。不delete锁，以免并发竞争。避免死锁，先排序shard，保证多个锁获取时，必须按顺序。\nAOF 过期时间的逻辑处理\n每次set的时候给通道发消息，异步routine处理\naof重写：redis 用的fork。godis用的aof生成副本，和ddl的异步更新有相同的意思\nZSet zset的实现。span实现rank，head虚节点作为首节点\ntype Level struct { forward *node // forward node has greater score span int64 } type node struct { Element backward *node level []*Level // level[0] is base level } type skiplist struct { header *node tail *node length int64 level int16 } Pipeline client 两个chan","title":"Godis"},{"content":"building kubebuilder init --domain tutorial.kubebuilder.io --repo tutorial.kubebuilder.io/project kubebuilder create api --group batch k--version v1 --kind CronJob config/manager: launch your controllers as pods in the cluster config/rbac: permissions required to run your controllers under their own service account config/default contains a Kustomize base for launching the controller in a standard configuration. main Scheme, Kinds 映射到 Go types\nmanager, 监控 controller，配置 cache client\nApi domain tutorial.kubebuilder.io kubebuilder create api --group batch --version v1 --kind CronJob 对应的apiVersion就是 batch.tutorial.kubebuilder.io/v1 type.go spec 注释\n//+kubebuilder:validation:MinLength=0 //+kubebuilder:validation:Minimum=0 // +optional status kind,list +kubebuilder:object:root. 生成kind\nSchemeBuilder.Register 注册\ncontroller rbac reconcile reconcile 实现了 Reader Writer的接口，可以直接 get List create delete\n1 using client to fetch CR\nr.Get(ctx, req.NamespacedName, \u0026amp;cronJob) r.List(ctx, \u0026amp;childJobs, client.InNamespace(req.Namespace), client.MatchingFields{jobOwnerKey: req.Name}) 2 根据自己逻辑 控制CR\nFinalizers add finalizer后要 update delete object: predelete logic → delete finalizer → update object pre-delete must be idempotent Watch 自己控制的 Own controllerutil.SetControllerReference\nManager builder 模式添加 Owns\nexternal builder模式 添加 Watch\nWebhook ","permalink":"https://homily707.github.io/posts/k8s/kubebuilder/","summary":"building kubebuilder init --domain tutorial.kubebuilder.io --repo tutorial.kubebuilder.io/project kubebuilder create api --group batch k--version v1 --kind CronJob config/manager: launch your controllers as pods in the cluster config/rbac: permissions required to run your controllers under their own service account config/default contains a Kustomize base for launching the controller in a standard configuration. main Scheme, Kinds 映射到 Go types\nmanager, 监控 controller，配置 cache client\nApi domain tutorial.kubebuilder.io kubebuilder create api --group batch --version v1 --kind CronJob 对应的apiVersion就是 batch.","title":"Kubebuilder"},{"content":"核心逻辑：StartReturningModel()\n使用chan来实现定时阻塞和阻塞。用5个通道来生命周期的管理 waitForGoroutines = func(withReadLoop bool) { if withReadLoop { select { case \u0026lt;-p.readLoopDone: case \u0026lt;-time.After(500 * time.Millisecond): // The read loop hangs, which means the input // cancelReader\u0026#39;s cancel function has returned true even // though it was not able to cancel the read. } } \u0026lt;-cmdLoopDone \u0026lt;-resizeLoopDone \u0026lt;-sigintLoopDone \u0026lt;-initSignalDone } /dev/tty 输入 ANSI 控制码，封装了各种操作 Go：select阻塞住等待SIGINT。 signal.Notify(sig, syscall.SIGINT) 多个协程中都在select中加入case \u0026lt;-p.ctx.Done(): ，要退出就一起退出 Go：从通道读取cmd 死循环读取msg和写入cmd，然后update。 ","permalink":"https://homily707.github.io/posts/go/bubbletea/","summary":"核心逻辑：StartReturningModel()\n使用chan来实现定时阻塞和阻塞。用5个通道来生命周期的管理 waitForGoroutines = func(withReadLoop bool) { if withReadLoop { select { case \u0026lt;-p.readLoopDone: case \u0026lt;-time.After(500 * time.Millisecond): // The read loop hangs, which means the input // cancelReader\u0026#39;s cancel function has returned true even // though it was not able to cancel the read. } } \u0026lt;-cmdLoopDone \u0026lt;-resizeLoopDone \u0026lt;-sigintLoopDone \u0026lt;-initSignalDone } /dev/tty 输入 ANSI 控制码，封装了各种操作 Go：select阻塞住等待SIGINT。 signal.Notify(sig, syscall.SIGINT) 多个协程中都在select中加入case \u0026lt;-p.ctx.Done(): ，要退出就一起退出 Go：从通道读取cmd 死循环读取msg和写入cmd，然后update。 ","title":"Bubbletea"},{"content":" ","permalink":"https://homily707.github.io/posts/db/tidb-0/","summary":" ","title":"Tidb-learning-0"},{"content":" 存储的设计 事务模型 查询引擎 复制 基础篇 外部视角 写多读少、低延时、高并发\n海量并发（分布式和单体的区别）\n高可靠\nrto 恢复时间 rpo 恢复点\n海量存储\n内部视角 客户端组件 + 单体 sharding jdbc 中间件 + 单体。 MyCat 应用层重构 + 单体 数据一致性 状态一致性\n强一致：mysql 全同步复制 弱一致：eventually consistency 操作一致性\n写后读 写入成功，异步复制，保证写入者能读到 单调读 读过的数据，不会消失。将用户和副本建立映射关系。 前缀 保证事件复制的因果关系 线性一致性 Linearizability 所有操作可以比较先后顺序。通过全局时钟建立全序关系。 线性一致性是描述历史记录的，而不是描述系统的。我们可以判断访问系统获取的一系列历史记录，来判断这个结果是不是线性一致，从而判断这个系统是否能实现线性一致。 因果一致 逻辑时钟，建立不那么准确的 全序关系 事务一致性 ACID\n一致性：整体目标\n持久性：数据丢失、故障容错。write ahead log， 多副本\n隔离性 异常现象 幻读： T1 查询两次， T2插入。T1第2次读结果集增大\n不可重复读： T1 查询两次， T2修改并提交。T1两次读结果不一致\n脏读： T1 查询两次， T2修改但未提交。T1两次读结果不一致\n隔离级别 未提交读： 脏读\n已提交读： 不可重复读\n可重复读： 会幻读\n快照隔离： write skew\n可串形化： 事务一个个执行\n架构 客户端通讯 查询处理器 事务管理器 进程管理器 辅助工具 单体数据库\n单体数据库（数据节点） + 协调节点 + 分片信息 + 全局时钟\nnewSql\n计算节点 + 存储节点 + 全局时钟 + 元数据管理\n时钟 spanner true time 物理时间 ，多时间源，多授时点\nTIDB TSO 中心授时 逻辑时钟。\nPD 组成raft组，提供授时服务，使用etcd + 预申请时间窗\n分片 hash 、 一致性hash。（无法处理业务热点，无法范围查询）\nrange 静态\nrange 动态\n动态分裂、合并 通过调度实现存储均衡、压力均衡。减少分布式事务 数据复制 元数据 TIKV 上node多个raft组存储分片信息。PD中心点读取kv心跳，获取当前分片信息。\n复制效率 raft的半数同意会导致效率低，且操作是按顺序apply的\nTIDB优化\nbatch，多个请求一起发送 pipeline，nextIndex parallelly，leader和follower的log并行apply apply，异步执行 分布式事务 原子性 TCC 应用层修改\n2PC 事务管理器，在多个节点上先分别prepare，确认后再commit\n问题：同步阻塞，事务管理器单点故障、commit失败数据不一致。\nPrecolator\n使用mvcc，新增版本。在新版本上加主锁和次锁，次锁指向主锁。\ncommit，删掉主锁即可。次锁记录，lazy 加载，异步删除\n事务延迟 延迟与写操作线性相关\n缓存提交写 隔离性 MVCC。分布式中通过全局事务管理器，维护递增事务id、提供全局快照\n存储方式\nappend-only 存储在表中 delta，独立存储增量 timetravel，独立存储全量 TIDB没有采用快照，读写阻塞\n不确定的时间窗口。\nspanner 写等待 cockroach 读等待\n悲观、乐观 乐观 rcvw 读 算 验证 写\n悲观 vrcw 验证 读 算 写 此处的读是拷贝加改写副本。写是提交\ntidb乐观锁，虽然每一行都悲观的。但是整体事务是没有有效性验证\n悲观，执行sql时，添加锁\n查询 存储过程\n难以调试\n自增主键\n连续递增，事务回滚时会失效 单调递增，并发过大时会失效 尾部热点， HTAP\nOLTP 和 OLAP 之间，需要ETL进行转换，存在时延。\nTiFlash存储分离，flash 作为raft的一个learner。每次查询前找leader实现同步。\n采用delta tree。\nKappa\nkafka加stream计算，快速加工\n计算下推 谓词下推\n分区索引\n全局索引\n查询引擎 火山模型\n向量化模型\n代码生成\nRUM read update memory 三者不能同时优化\nB tree 写放大，实际写比写的操作要多 存储不连续\nLSM tree Tiered\n先写内存，直接返回 内存 flush 进入 sstable sstable compact 合并 读放大，要同时读多个sstable\n写放大，compact操作性能消耗很高\nleveled compact\nL0 可重叠\n整理形成不重叠的L1\n然后部分compact\nWiscKey value单独存储\n","permalink":"https://homily707.github.io/posts/db/distribute-30/","summary":"存储的设计 事务模型 查询引擎 复制 基础篇 外部视角 写多读少、低延时、高并发\n海量并发（分布式和单体的区别）\n高可靠\nrto 恢复时间 rpo 恢复点\n海量存储\n内部视角 客户端组件 + 单体 sharding jdbc 中间件 + 单体。 MyCat 应用层重构 + 单体 数据一致性 状态一致性\n强一致：mysql 全同步复制 弱一致：eventually consistency 操作一致性\n写后读 写入成功，异步复制，保证写入者能读到 单调读 读过的数据，不会消失。将用户和副本建立映射关系。 前缀 保证事件复制的因果关系 线性一致性 Linearizability 所有操作可以比较先后顺序。通过全局时钟建立全序关系。 线性一致性是描述历史记录的，而不是描述系统的。我们可以判断访问系统获取的一系列历史记录，来判断这个结果是不是线性一致，从而判断这个系统是否能实现线性一致。 因果一致 逻辑时钟，建立不那么准确的 全序关系 事务一致性 ACID\n一致性：整体目标\n持久性：数据丢失、故障容错。write ahead log， 多副本\n隔离性 异常现象 幻读： T1 查询两次， T2插入。T1第2次读结果集增大\n不可重复读： T1 查询两次， T2修改并提交。T1两次读结果不一致\n脏读： T1 查询两次， T2修改但未提交。T1两次读结果不一致\n隔离级别 未提交读： 脏读","title":"分布式数据库笔记"},{"content":"原理理解 lowbit 就是数字的最低位 11000 -》 1000\n在树状数组中，我们用lowbit表示这个下标的sum，包含了几个数。比如 11010 的 lowbit 为 10，即他管辖了 （11000，11010] 这两个数的和，左开右闭。所以当我们要求【0，x】的和的时候，就是求[0，x-lowbit(x)]+（x-lowbit(x),x] ，后者我们已经存储好了，前者通过迭代的方式求出。因为是左开右闭，我们实际只能求到（0,x]，所以我们要能额外添加一个index为0的，值也为0的值。所以在实现的时候，真正的数字是从下标1开始的。\n一个数加上自己lowbit 11010 + 10 = 11100， 实现的就是最低位的进位，而这个进位所得到的数，一定是管辖自己的数。\n实现要点 sum数组的下标和原始数组的下标存在加1的关系 lowbit 用 x\u0026amp;-x实现 show me the code type BinaryIndexTree struct { Data []int SumSlice []int Len int } func (bit *BinaryIndexTree) Init(array []int) { bit.Len = len(array) bit.Data = array bit.SumSlice = make([]int, bit.Len+1) for i, num := range array { bit.addInSum(i, num) } } func (bit *BinaryIndexTree) Add(index int, delta int) { bit.Data[index] += delta bit.addInSum(index, delta) } func (bit *BinaryIndexTree) Sum(left int, right int) int { return bit.getPreSum(right-1) - bit.getPreSum(left-1) } func (bit *BinaryIndexTree) addInSum(index int, delta int) { index++ for index \u0026lt;= bit.Len { bit.SumSlice[index] += delta index += lowbit(index) } } func (bit BinaryIndexTree) getPreSum(pos int) int { index := pos + 1 sum := 0 for index \u0026gt; 0 { sum += bit.SumSlice[index] index -= lowbit(index) } return sum } func lowbit(x int) int { return x \u0026amp; (-x) } ","permalink":"https://homily707.github.io/posts/algo/binaryindextree/","summary":"原理理解 lowbit 就是数字的最低位 11000 -》 1000\n在树状数组中，我们用lowbit表示这个下标的sum，包含了几个数。比如 11010 的 lowbit 为 10，即他管辖了 （11000，11010] 这两个数的和，左开右闭。所以当我们要求【0，x】的和的时候，就是求[0，x-lowbit(x)]+（x-lowbit(x),x] ，后者我们已经存储好了，前者通过迭代的方式求出。因为是左开右闭，我们实际只能求到（0,x]，所以我们要能额外添加一个index为0的，值也为0的值。所以在实现的时候，真正的数字是从下标1开始的。\n一个数加上自己lowbit 11010 + 10 = 11100， 实现的就是最低位的进位，而这个进位所得到的数，一定是管辖自己的数。\n实现要点 sum数组的下标和原始数组的下标存在加1的关系 lowbit 用 x\u0026amp;-x实现 show me the code type BinaryIndexTree struct { Data []int SumSlice []int Len int } func (bit *BinaryIndexTree) Init(array []int) { bit.Len = len(array) bit.Data = array bit.SumSlice = make([]int, bit.Len+1) for i, num := range array { bit.addInSum(i, num) } } func (bit *BinaryIndexTree) Add(index int, delta int) { bit.","title":"树状数组"}]